{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# RAG with LlamaStack - Complete Example\n",
        "\n",
        "This notebook demonstrates how to build a Retrieval-Augmented Generation (RAG) system using LlamaStack.\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "uv pip install llama-stack-client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'llama_stack_client'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[12], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# RAG with LlamaStack - Complete Example\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mllama_stack_client\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LlamaStackClient\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01muuid\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Check if environment variable is set\u001b[39;00m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'llama_stack_client'"
          ]
        }
      ],
      "source": [
        "# RAG with LlamaStack - Complete Example\n",
        "import os\n",
        "from llama_stack_client import LlamaStackClient\n",
        "import uuid\n",
        "\n",
        "# Check if environment variable is set\n",
        "if 'LLAMA_STACK_PORT' not in os.environ:\n",
        "    print(\"‚ùå LLAMA_STACK_PORT environment variable not set!\")\n",
        "    print(\"Please run: export LLAMA_STACK_PORT=8321\")\n",
        "    raise EnvironmentError(\"LLAMA_STACK_PORT not set\")\n",
        "else:\n",
        "    print(f\"‚úÖ LLAMA_STACK_PORT set to {os.environ['LLAMA_STACK_PORT']}\")\n",
        "\n",
        "# Create client\n",
        "client = LlamaStackClient(base_url=f\"http://localhost:{os.environ['LLAMA_STACK_PORT']}\")\n",
        "\n",
        "try:\n",
        "    # Test connection first\n",
        "    models = client.models.list()\n",
        "    print(f\"‚úÖ Connected! Found {len(models)} models:\")\n",
        "    for model in models:\n",
        "        print(f\"  - {model.model_type}: {model.identifier}\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Connection Error: {e}\")\n",
        "    print(\"Make sure LlamaStack server is running on port\", os.environ.get('LLAMA_STACK_PORT', '8321'))\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Register a vector database with unique ID\n",
        "vector_db_id = f\"my_documents_{uuid.uuid4().hex[:8]}\"\n",
        "print(f\"üìä Creating vector database: {vector_db_id}\")\n",
        "\n",
        "try:\n",
        "    response = client.vector_dbs.register(\n",
        "        vector_db_id=vector_db_id,\n",
        "        embedding_model=\"all-MiniLM-L6-v2\",\n",
        "        embedding_dimension=384,\n",
        "        provider_id=\"faiss\",\n",
        "    )\n",
        "    print(\"‚úÖ Vector DB registered successfully!\")\n",
        "    print(f\"Response: {response}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Vector DB Registration Error: {e}\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìù Inserting 3 document chunks...\n",
            "‚ùå Document Insertion Error: name 'client' is not defined\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'client' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[9], line 31\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müìù Inserting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(chunks)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m document chunks...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 31\u001b[0m     \u001b[43mclient\u001b[49m\u001b[38;5;241m.\u001b[39mvector_io\u001b[38;5;241m.\u001b[39minsert(vector_db_id\u001b[38;5;241m=\u001b[39mvector_db_id, chunks\u001b[38;5;241m=\u001b[39mchunks)\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚úÖ Document chunks inserted successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "\u001b[0;31mNameError\u001b[0m: name 'client' is not defined"
          ]
        }
      ],
      "source": [
        "# Insert sample documents about AI and RAG\n",
        "chunks = [\n",
        "    {\n",
        "        \"content\": \"Retrieval-Augmented Generation (RAG) is a technique that combines information retrieval with text generation. It allows language models to access external knowledge sources to provide more accurate and up-to-date responses.\",\n",
        "        \"mime_type\": \"text/plain\",\n",
        "        \"metadata\": {\n",
        "            \"document_id\": \"rag_intro\",\n",
        "            \"topic\": \"RAG basics\",\n",
        "        },\n",
        "    },\n",
        "    {\n",
        "        \"content\": \"LlamaStack is an open-source platform that provides standardized APIs for building AI applications. It supports various providers for inference, vector storage, and other AI capabilities.\",\n",
        "        \"mime_type\": \"text/plain\",\n",
        "        \"metadata\": {\n",
        "            \"document_id\": \"llamastack_info\",\n",
        "            \"topic\": \"LlamaStack overview\",\n",
        "        },\n",
        "    },\n",
        "    {\n",
        "        \"content\": \"Vector databases store high-dimensional embeddings that represent the semantic meaning of text. This enables semantic search and retrieval based on meaning rather than exact keyword matches.\",\n",
        "        \"mime_type\": \"text/plain\",\n",
        "        \"metadata\": {\n",
        "            \"document_id\": \"vector_db_info\",\n",
        "            \"topic\": \"Vector databases\",\n",
        "        },\n",
        "    },\n",
        "]\n",
        "\n",
        "print(f\"üìù Inserting {len(chunks)} document chunks...\")\n",
        "try:\n",
        "    client.vector_io.insert(vector_db_id=vector_db_id, chunks=chunks)\n",
        "    print(\"‚úÖ Document chunks inserted successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Document Insertion Error: {e}\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test queries\n",
        "test_queries = [\n",
        "    \"What is RAG?\",\n",
        "    \"Tell me about LlamaStack\",  \n",
        "    \"How do vector databases work?\"\n",
        "]\n",
        "\n",
        "print(\"üîç Testing RAG queries:\")\n",
        "for query in test_queries:\n",
        "    print(f\"\\nQuery: '{query}'\")\n",
        "    try:\n",
        "        chunks_response = client.vector_io.query(\n",
        "            vector_db_id=vector_db_id, \n",
        "            query=query,\n",
        "            max_chunks=2\n",
        "        )\n",
        "        print(f\"Found {len(chunks_response.chunks)} relevant chunks:\")\n",
        "        for i, chunk in enumerate(chunks_response.chunks, 1):\n",
        "            print(f\"  {i}. Score: {chunk.score:.3f}\")\n",
        "            print(f\"     Content: {chunk.content[:100]}...\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Query Error: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clean up\n",
        "print(f\"üßπ Cleaning up vector database...\")\n",
        "try:\n",
        "    client.vector_dbs.unregister(vector_db_id)\n",
        "    print(\"‚úÖ Cleanup completed!\")\n",
        "    print(\"\\nüéâ RAG system is working perfectly!\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Cleanup Error: {e}\")\n",
        "    print(\"You may need to manually clean up the vector database\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
