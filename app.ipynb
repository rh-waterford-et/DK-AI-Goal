{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# RAG with LlamaStack - Complete Working Example\n",
    "\n",
    "This notebook demonstrates how to build a Retrieval-Augmented Generation (RAG) system using LlamaStack.\n",
    "\n",
    "## Requirements\n",
    "- LlamaStack server running on port 8321\n",
    "- Ollama with llama3.2:3b model\n",
    "- Python environment with required packages\n",
    "\n",
    "## What this notebook does:\n",
    "1. ‚úÖ Connect to LlamaStack and test the connection\n",
    "2. ‚úÖ Create a vector database for document storage  \n",
    "3. ‚úÖ Insert sample documents about AI/RAG topics\n",
    "4. ‚úÖ Perform semantic search queries\n",
    "5. ‚úÖ Clean up resources\n",
    "\n",
    "**Note:** Make sure to run the first cell completely before running other cells!\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Setup Instructions\n",
    "\n",
    "If this is your first time running this notebook, make sure:\n",
    "\n",
    "1. **LlamaStack server is running:**\n",
    "   ```bash\n",
    "   INFERENCE_MODEL=llama3.2:3b uv run --with llama-stack llama stack build --template ollama --image-type venv --run\n",
    "   ```\n",
    "\n",
    "2. **Using correct kernel:** Select \"AI Goal RAG Environment\" from the kernel dropdown (if available)\n",
    "\n",
    "3. **Manual installation (if needed):**\n",
    "   ```bash\n",
    "   uv add llama-stack-client fire requests\n",
    "   ```\n",
    "\n",
    "The first code cell below will attempt to auto-install missing packages if needed."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET http://localhost:8321/v1/models \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python executable: /Users/dekelly/AI-Goal/.venv/bin/python3\n",
      "Environment correct: True\n",
      "‚úÖ llama_stack_client imported successfully!\n",
      "‚úÖ Client created for http://localhost:8321\n",
      "‚úÖ Connected! Found 2 models:\n",
      "  - llm: llama3.2:3b\n",
      "  - embedding: all-MiniLM-L6-v2\n",
      "\n",
      "üéâ Setup complete! All variables stored for other cells.\n"
     ]
    }
   ],
   "source": [
    "# RAG with LlamaStack - Complete Setup and Connection Test\n",
    "import os\n",
    "import sys\n",
    "import uuid\n",
    "\n",
    "# Set environment variable directly in notebook\n",
    "os.environ['LLAMA_STACK_PORT'] = '8321'\n",
    "\n",
    "# Check Python environment for debugging\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "print(f\"Environment correct: {'/Users/dekelly/AI-Goal/.venv' in sys.executable}\")\n",
    "\n",
    "try:\n",
    "    from llama_stack_client import LlamaStackClient\n",
    "    print(\"‚úÖ llama_stack_client imported successfully!\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Import error: {e}\")\n",
    "    print(\"Trying to install in current kernel...\")\n",
    "    \n",
    "    # Install packages in current kernel if missing\n",
    "    import subprocess\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"llama-stack-client\", \"fire\", \"requests\"])\n",
    "        from llama_stack_client import LlamaStackClient\n",
    "        print(\"‚úÖ Successfully installed and imported!\")\n",
    "    except Exception as install_error:\n",
    "        print(f\"‚ùå Installation failed: {install_error}\")\n",
    "        print(\"Please make sure you're using the 'AI Goal RAG Environment' kernel!\")\n",
    "        raise\n",
    "\n",
    "# Create client\n",
    "client = LlamaStackClient(base_url=f\"http://localhost:{os.environ['LLAMA_STACK_PORT']}\")\n",
    "print(f\"‚úÖ Client created for http://localhost:{os.environ['LLAMA_STACK_PORT']}\")\n",
    "\n",
    "try:\n",
    "    # Test connection\n",
    "    models = client.models.list()\n",
    "    print(f\"‚úÖ Connected! Found {len(models)} models:\")\n",
    "    for model in models:\n",
    "        print(f\"  - {model.model_type}: {model.identifier}\")\n",
    "    \n",
    "    # Store variables globally for other cells\n",
    "    globals()['client'] = client\n",
    "    globals()['uuid'] = uuid\n",
    "    globals()['LlamaStackClient'] = LlamaStackClient\n",
    "    \n",
    "    print(\"\\nüéâ Setup complete! All variables stored for other cells.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Connection Error: {e}\")\n",
    "    print(\"Make sure LlamaStack server is running on port 8321\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8321/v1/vector-dbs \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Creating vector database: my_documents_28fd79ac\n",
      "‚úÖ Vector DB registered successfully!\n",
      "Response: VectorDBRegisterResponse(embedding_dimension=384, embedding_model='all-MiniLM-L6-v2', identifier='my_documents_28fd79ac', provider_id='faiss', type='vector_db', provider_resource_id='my_documents_28fd79ac', owner={'principal': '', 'attributes': {}})\n"
     ]
    }
   ],
   "source": [
    "# Register a vector database with unique ID\n",
    "vector_db_id = f\"my_documents_{uuid.uuid4().hex[:8]}\"\n",
    "print(f\"üìä Creating vector database: {vector_db_id}\")\n",
    "\n",
    "try:\n",
    "    response = client.vector_dbs.register(\n",
    "        vector_db_id=vector_db_id,\n",
    "        embedding_model=\"all-MiniLM-L6-v2\",\n",
    "        embedding_dimension=384,\n",
    "        provider_id=\"faiss\",\n",
    "    )\n",
    "    print(\"‚úÖ Vector DB registered successfully!\")\n",
    "    print(f\"Response: {response}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Vector DB Registration Error: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8321/v1/vector-io/insert \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Inserting 3 document chunks...\n",
      "‚úÖ Document chunks inserted successfully!\n"
     ]
    }
   ],
   "source": [
    "# Insert sample documents about AI and RAG\n",
    "chunks = [\n",
    "    {\n",
    "        \"content\": \"Retrieval-Augmented Generation (RAG) is a technique that combines information retrieval with text generation. It allows language models to access external knowledge sources to provide more accurate and up-to-date responses.\",\n",
    "        \"mime_type\": \"text/plain\",\n",
    "        \"metadata\": {\n",
    "            \"document_id\": \"rag_intro\",\n",
    "            \"topic\": \"RAG basics\",\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"content\": \"LlamaStack is an open-source platform that provides standardized APIs for building AI applications. It supports various providers for inference, vector storage, and other AI capabilities.\",\n",
    "        \"mime_type\": \"text/plain\",\n",
    "        \"metadata\": {\n",
    "            \"document_id\": \"llamastack_info\",\n",
    "            \"topic\": \"LlamaStack overview\",\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"content\": \"Vector databases store high-dimensional embeddings that represent the semantic meaning of text. This enables semantic search and retrieval based on meaning rather than exact keyword matches.\",\n",
    "        \"mime_type\": \"text/plain\",\n",
    "        \"metadata\": {\n",
    "            \"document_id\": \"vector_db_info\",\n",
    "            \"topic\": \"Vector databases\",\n",
    "        },\n",
    "    },\n",
    "]\n",
    "\n",
    "print(f\"üìù Inserting {len(chunks)} document chunks...\")\n",
    "try:\n",
    "    client.vector_io.insert(vector_db_id=vector_db_id, chunks=chunks)\n",
    "    print(\"‚úÖ Document chunks inserted successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Document Insertion Error: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8321/v1/vector-io/query \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8321/v1/vector-io/query \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8321/v1/vector-io/query \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Testing RAG queries:\n",
      "\n",
      "Query: 'What is RAG?'\n",
      "Found 3 relevant chunks:\n",
      "  1. Score: 0.000\n",
      "     Content: Retrieval-Augmented Generation (RAG) is a technique that combines information retrieval with text ge...\n",
      "     Topic: RAG basics\n",
      "  2. Score: 0.000\n",
      "     Content: LlamaStack is an open-source platform that provides standardized APIs for building AI applications. ...\n",
      "     Topic: LlamaStack overview\n",
      "  3. Score: 0.000\n",
      "     Content: Vector databases store high-dimensional embeddings that represent the semantic meaning of text. This...\n",
      "     Topic: Vector databases\n",
      "\n",
      "Query: 'Tell me about LlamaStack'\n",
      "Found 3 relevant chunks:\n",
      "  1. Score: 0.000\n",
      "     Content: LlamaStack is an open-source platform that provides standardized APIs for building AI applications. ...\n",
      "     Topic: LlamaStack overview\n",
      "  2. Score: 0.000\n",
      "     Content: Retrieval-Augmented Generation (RAG) is a technique that combines information retrieval with text ge...\n",
      "     Topic: RAG basics\n",
      "  3. Score: 0.000\n",
      "     Content: Vector databases store high-dimensional embeddings that represent the semantic meaning of text. This...\n",
      "     Topic: Vector databases\n",
      "\n",
      "Query: 'How do vector databases work?'\n",
      "Found 3 relevant chunks:\n",
      "  1. Score: 0.000\n",
      "     Content: Vector databases store high-dimensional embeddings that represent the semantic meaning of text. This...\n",
      "     Topic: Vector databases\n",
      "  2. Score: 0.000\n",
      "     Content: LlamaStack is an open-source platform that provides standardized APIs for building AI applications. ...\n",
      "     Topic: LlamaStack overview\n",
      "  3. Score: 0.000\n",
      "     Content: Retrieval-Augmented Generation (RAG) is a technique that combines information retrieval with text ge...\n",
      "     Topic: RAG basics\n"
     ]
    }
   ],
   "source": [
    "# Test queries\n",
    "test_queries = [\n",
    "    \"What is RAG?\",\n",
    "    \"Tell me about LlamaStack\",  \n",
    "    \"How do vector databases work?\"\n",
    "]\n",
    "\n",
    "print(\"üîç Testing RAG queries:\")\n",
    "for query in test_queries:\n",
    "    print(f\"\\nQuery: '{query}'\")\n",
    "    try:\n",
    "        chunks_response = client.vector_io.query(\n",
    "            vector_db_id=vector_db_id, \n",
    "            query=query\n",
    "        )\n",
    "        print(f\"Found {len(chunks_response.chunks)} relevant chunks:\")\n",
    "        for i, chunk in enumerate(chunks_response.chunks, 1):\n",
    "            # Handle different score formats\n",
    "            score = getattr(chunk, 'score', 0.0)\n",
    "            print(f\"  {i}. Score: {score:.3f}\")\n",
    "            print(f\"     Content: {chunk.content[:100]}...\")\n",
    "            if hasattr(chunk, 'metadata') and chunk.metadata:\n",
    "                print(f\"     Topic: {chunk.metadata.get('topic', 'Unknown')}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Query Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET http://localhost:8321/v1/models \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8321/v1/agents \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:8321/v1/tools?toolgroup_id=builtin%3A%3Arag%2Fknowledge_search \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8321/v1/agents/79e598e6-f0dd-463c-99b4-18f5472b75ab/session \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8321/v1/agents/79e598e6-f0dd-463c-99b4-18f5472b75ab/session/ea5d1fc0-0fa8-4988-9870-3d9a343b6d9c/turn \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Creating RAG Agent for interactive Q&A...\n",
      "‚úÖ RAG Agent created with model: llama3.2:3b\n",
      "‚úÖ Session created: ea5d1fc0-0fa8-4988-9870-3d9a343b6d9c\n",
      "\\nüß™ Testing agent with a question...\n",
      "Question: What are the main benefits of using RAG in AI applications?\n",
      "\\nü§ñ Agent Response:\\nRAG (Retrieval-Augmented Generation) has several main benefits in AI applications:\n",
      "\n",
      "1. **Improved accuracy**: By accessing external knowledge sources, RAG models can provide more accurate and up-to-date responses to user queries.\n",
      "2. **Enhanced contextual understanding**: RAG allows language models to understand the context of a query and retrieve relevant information from external sources, leading to more informed and precise responses.\n",
      "3. **Increased efficiency**: RAG can reduce the need for manual knowledge graph construction and maintenance, making it easier to integrate with existing applications and systems.\n",
      "4. **Scalability**: RAG can handle large volumes of data and scale to meet the needs of complex AI applications.\n",
      "\n",
      "These benefits enable RAG to be a powerful tool in various AI applications, including but not limited to chatbots, virtual assistants, and content generation systems.\n",
      "\\n‚úÖ Agent stored globally - you can ask more questions!\n"
     ]
    }
   ],
   "source": [
    "# Optional: Create RAG Agent for Interactive Q&A\n",
    "print(\"ü§ñ Creating RAG Agent for interactive Q&A...\")\n",
    "\n",
    "try:\n",
    "    from llama_stack_client import Agent\n",
    "    \n",
    "    # Get the LLM model\n",
    "    models = client.models.list()\n",
    "    llm_model = next(m for m in models if m.model_type == \"llm\")\n",
    "    \n",
    "    # Create an agent with RAG capabilities\n",
    "    agent = Agent(\n",
    "        client,\n",
    "        model=llm_model.identifier,\n",
    "        instructions=\"You are a helpful AI assistant with access to knowledge about RAG, LlamaStack, and vector databases. Use the search tool to find relevant information before answering questions.\",\n",
    "        tools=[\n",
    "            {\n",
    "                \"name\": \"builtin::rag/knowledge_search\",\n",
    "                \"args\": {\"vector_db_ids\": [vector_db_id]}\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ RAG Agent created with model: {llm_model.identifier}\")\n",
    "    \n",
    "    # Create a session\n",
    "    session_id = agent.create_session(\"rag_demo_session\")\n",
    "    print(f\"‚úÖ Session created: {session_id}\")\n",
    "    \n",
    "    # Test the agent\n",
    "    print(\"\\\\nüß™ Testing agent with a question...\")\n",
    "    question = \"What are the main benefits of using RAG in AI applications?\"\n",
    "    print(f\"Question: {question}\")\n",
    "    \n",
    "    response = agent.create_turn(\n",
    "        messages=[{\"role\": \"user\", \"content\": question}],\n",
    "        session_id=session_id,\n",
    "        stream=False\n",
    "    )\n",
    "    \n",
    "    print(f\"\\\\nü§ñ Agent Response:\\\\n{response.output_message.content}\")\n",
    "    \n",
    "    # Store agent for further use\n",
    "    globals()['agent'] = agent\n",
    "    globals()['session_id'] = session_id\n",
    "    print(\"\\\\n‚úÖ Agent stored globally - you can ask more questions!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Agent creation failed: {e}\")\n",
    "    print(\"Don't worry - basic RAG functionality still works perfectly!\")\n",
    "    print(\"This is just an optional advanced feature.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: DELETE http://localhost:8321/v1/vector-dbs/my_documents_28fd79ac \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Cleaning up vector database...\n",
      "‚úÖ Cleanup completed!\n",
      "\n",
      "üéâ RAG system is working perfectly!\n"
     ]
    }
   ],
   "source": [
    "# Clean up\n",
    "print(f\"üßπ Cleaning up vector database...\")\n",
    "try:\n",
    "    client.vector_dbs.unregister(vector_db_id)\n",
    "    print(\"‚úÖ Cleanup completed!\")\n",
    "    print(\"\\nüéâ RAG system is working perfectly!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Cleanup Error: {e}\")\n",
    "    print(\"You may need to manually clean up the vector database\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
